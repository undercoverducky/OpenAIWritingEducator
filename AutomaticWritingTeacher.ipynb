{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/undercoverducky/OpenAIWritingEducator/blob/main/AutomaticWritingTeacher.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Prerequisites\n"
      ],
      "metadata": {
        "id": "Z0FlAqpfuRem"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52qPTwHyinGv"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install streamlit\n",
        "!pip install tiktoken\n",
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prototype Source code and Testing\n"
      ],
      "metadata": {
        "id": "ZUtYON4oua8O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQIqX52am5LA"
      },
      "outputs": [],
      "source": [
        "import langchain\n",
        "import os, streamlit as st\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.cache import InMemoryCache\n",
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    PromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "\n",
        "frq_quality_rubric = [\"Can the question be answered using only evidence from the context above?\",\n",
        "                      \"Does the question test understanding of the information presented in the context?\"]\n",
        "feedback_quality_rubric = [\"contradict itself\",\n",
        "                           \"repeat the same points\"]\n",
        "\n",
        "class TeachingStaff:\n",
        "\n",
        "  def __init__(self, api_key, standard, topic=None, rubric=None, enable_qa=False):\n",
        "    chatllm = ChatOpenAI(openai_api_key=api_key, model_name='gpt-4') #model_name='gpt-4'\n",
        "    self.llm = OpenAI(openai_api_key=api_key, model_name='gpt-4') #model_name='gpt-4'\n",
        "\n",
        "    if topic != None:\n",
        "      self.set_topic(topic)\n",
        "    self.standard = standard\n",
        "\n",
        "    self.knowledge_gen_chain, self.question_asking_chain, \\\n",
        "    self.t_chain, self.s_chain, self.quality_chain = self.initialize_agents(chatllm)\n",
        "\n",
        "    if rubric != None:\n",
        "      self.rubric = rubric\n",
        "    else:\n",
        "      self.rubric = self.generate_rubric()\n",
        "\n",
        "    self.enabled_qa = enable_qa\n",
        "\n",
        "  def set_topic(self, topic):\n",
        "    self.topic = topic\n",
        "\n",
        "  def get_rubric(self):\n",
        "    return self.rubric\n",
        "\n",
        "  def initialize_agents(self, llm):\n",
        "    # knowledge generation chain\n",
        "    kg_template = \"\"\"Mary is an super-intelligent, advanced AI task executor that posseses accurate knowledge on every topic.\n",
        "     It does not mention itself or admit its nature as an AI.\n",
        "     It uses the voice of a primary or secondary source.\n",
        "     It does not use first or second person except when quoting a source.\n",
        "     It fulfills requests exactly and concisely\n",
        "     For the following requests, you will respond and do tasks as Mary.\n",
        "\n",
        "     {chat_history}\n",
        "     Human: {task}\n",
        "     Mary:\"\"\"\n",
        "    kg_prompt = PromptTemplate(input_variables=[\"chat_history\", \"task\"], template=kg_template)\n",
        "    kg_memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "    kg_chain = LLMChain(llm=llm, prompt=kg_prompt, verbose=False, memory=kg_memory)\n",
        "\n",
        "    # question asking chain\n",
        "    qa_template = f\"\"\"John is an super-intelligent question asking AI with critical reading and thinking skills.\n",
        "     It does not mention itself or admit its nature as an AI.\n",
        "     It does not use first or second person except when quoting a source.\n",
        "     It specializes in generating insightful questions that test understanding of a text passage.\n",
        "     Its questions expect the response to exhibit the standard: {self.standard}.\n",
        "     For the following requests, you will respond and do tasks as John.\\n\\n\"\"\" + \\\n",
        "     \"\"\"\n",
        "     {chat_history}\n",
        "     Human: {task}\n",
        "     John:\"\"\"\n",
        "    qa_prompt = PromptTemplate(input_variables=[\"chat_history\", \"task\"], template=qa_template)\n",
        "    qa_memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "    qa_chain = LLMChain(llm=llm, prompt=qa_prompt, verbose=False, memory=qa_memory)\n",
        "\n",
        "    # teaching chain\n",
        "    t_template = \"\"\"Susan is an super-intelligent, advanced educational AI that is an expert at teaching students.\n",
        "     It is knowledgable in all common student mistakes.\n",
        "     It does not mention itself or admit its nature as an AI.\n",
        "     It does not use first or second person except when quoting a source.\n",
        "     It fulfills requests exactly and concisely\n",
        "     It is extremely competent in evaluating free response questions, identifying false information,\n",
        "     and providing the best advice for students to improve their writing with respect to a rubric.\n",
        "     For the following requests, you will respond and do tasks as Susan.\n",
        "\n",
        "     {chat_history}\n",
        "     Human: {task}\n",
        "     Susan:\"\"\"\n",
        "    t_prompt = PromptTemplate(input_variables=[\"chat_history\", \"task\"], template=t_template)\n",
        "    t_memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "    t_chain = LLMChain(llm=llm, prompt=t_prompt, verbose=False, memory=t_memory)\n",
        "\n",
        "    # student chain (simulate student for testing)\n",
        "    s_template = \"\"\"Zach is a human 4th grade student that is doing a writing assignment.\n",
        "    For the following requests, you will respond and write as Zach.\n",
        "\n",
        "    {chat_history}\n",
        "    {task}\n",
        "    Zach:\"\"\"\n",
        "    s_prompt = PromptTemplate(input_variables=[\"chat_history\", \"task\"], template=s_template)\n",
        "    s_memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "    s_chain = LLMChain(llm=llm, prompt=s_prompt, verbose=False, memory=s_memory)\n",
        "\n",
        "    # quality assurance agent\n",
        "    quality_template = \"\"\"Jan is an advanced teaching quality assurance AI that is an expert at\n",
        "    editing educational text to promote concise, helpful, and easy to understand material for students.\n",
        "    It does not mention itself or admit its nature as an AI.\n",
        "    It fulfills requests exactly and concisely without repeating the request.\n",
        "    It does not use first or second person except when quoting a source.\n",
        "    For the folling requests, you will respond as Jan.\n",
        "\n",
        "    {chat_history}\n",
        "    {task}\n",
        "    Jan:\"\"\"\n",
        "    quality_prompt = PromptTemplate(input_variables=[\"chat_history\", \"task\"], template=quality_template)\n",
        "    quality_memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "    quality_chain = LLMChain(llm=llm, prompt=quality_prompt, verbose=False, memory=quality_memory)\n",
        "\n",
        "    return kg_chain, qa_chain, t_chain, s_chain, quality_chain\n",
        "\n",
        "  def generate_FRQ(self, max_retry=2):\n",
        "    intro, context = self.generate_intro_context()\n",
        "    question = self.generate_question(context)\n",
        "    passed_qa = False\n",
        "    tries = 0\n",
        "    while(not passed_qa and self.enabled_qa and tries <= max_retry):\n",
        "      passed_qa, failed_check = self.check_FRQ_quality(intro + \"\\n\" + context, question)\n",
        "      if not passed_qa:\n",
        "        print(\"discarded question: \" + question + \"\\n due to failing to pass: \" + failed_check + \"\\n\")\n",
        "        question = self.generate_question(context)\n",
        "        tries = tries + 1\n",
        "\n",
        "    return intro + \"\\n\" + context, question\n",
        "\n",
        "  def check_FRQ_quality(self, context, question):\n",
        "    prompt = f\"\"\"Refer to the following context and question for the following requests:\n",
        "    <CONTEXT>\n",
        "    {context}\n",
        "    </CONTEXT>\n",
        "    <QUESTION>\n",
        "    {question}\n",
        "    </QUESTION>\n",
        "    \"\"\"\n",
        "    self.quality_chain.predict(task=prompt)\n",
        "\n",
        "    for check in frq_quality_rubric:\n",
        "      if (\"YES\" not in self.quality_chain.predict(task=check + \" Answer with one word YES or NO.\")):\n",
        "        return False, check\n",
        "    return True, \"\"\n",
        "\n",
        "  def check_feedback_quality(self, feedback):\n",
        "    prompt = f\"\"\"Refer to the following feedback for the following requests:\n",
        "    <FEEDBACK>\n",
        "    {feedback}\n",
        "    </FEEDBACK>\n",
        "    \"\"\"\n",
        "    self.quality_chain.predict(task=prompt)\n",
        "\n",
        "    for check in feedback_quality_rubric:\n",
        "      if (\"YES\" in self.quality_chain.predict(task=\"Does the feedback \" + check + \"? Answer with one word YES or NO.\")):\n",
        "        return False, check\n",
        "    return True, \"\"\n",
        "\n",
        "  def generate_intro_context(self):\n",
        "    intro_prompt = f\"Write a short 2 sentence introduction for the topic '{self.topic}'\"\n",
        "    intro = self.knowledge_gen_chain.predict(task=intro_prompt)\n",
        "    knowledge_prompt = f\"generate 8 facts related to the topic '{self.topic}' which could follow your introduction\"\n",
        "    knowledge = self.knowledge_gen_chain.predict(task=knowledge_prompt)\n",
        "    context_prompt = f\"generate 3 paragraphs about the topic naturally utilizing the above facts incorporating quotes if necessary\"\n",
        "    context = self.knowledge_gen_chain.predict(task=context_prompt)\n",
        "    return intro, context\n",
        "\n",
        "  def generate_question(self, context):\n",
        "    prompt = f\"Generate an open-ended question which can be answered solely by drawing evidence from the context:\\n'{context}'\"\n",
        "    question = self.question_asking_chain.predict(task=prompt)\n",
        "    return question\n",
        "\n",
        "  def generate_rubric(self):\n",
        "    prompt = \\\n",
        "    f\"\"\"Concisely generate a rubric for grading an essay answer based on how well it demonstrates the core standard '{self.standard}'.\n",
        "    It should assign a score from 1-3 and give criteria for meeting each score cutoff. \"\"\"\n",
        "    return self.t_chain.predict(task=prompt)\n",
        "\n",
        "  def generate_response(self, frq):\n",
        "    prompt = f\"Answer the following question in paragraph form:\\n {frq}\"\n",
        "    return self.s_chain.predict(task=prompt)\n",
        "\n",
        "  def try_n_times(self, prompt, condition, n):\n",
        "    for i in range(0, n):\n",
        "      if condition(self.llm(prompt)):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "  def evaluate_correctness(self, context, q, student_response):\n",
        "    sentences = sent_tokenize(student_response.strip())\n",
        "    correct = True\n",
        "    incorrect_sentences = []\n",
        "    for sentence in sentences:\n",
        "      correctness = f\"\"\"Identify claims from the response and and evaluate accuracy of each using evidence from the context step by step. Evidence from the context is not needed if the claim is common sense. Return a final answer of CORRECT if all claims are accurate, and INCORRECT otherwise..\n",
        "      Example1:\n",
        "      <CONTEXT>\n",
        "      </CONTEXT>\n",
        "      <RESPONSE>\n",
        "      During the Scramble for Africa, European powers justified their colonization efforts by claiming to bring civilization, Islam, and economic development to the continent.\n",
        "      </RESPONSE>\n",
        "      Evaluation:\n",
        "      Claim 1: European powers justified their colonization efforts by bringing civilization to the continent. \\n\\nAccuracy: CORRECT. The text states that European powers \"justified their colonization efforts by claiming to bring civilization, Christianity, and economic development to the continent.\" \\n\\nClaim 2: European powers justified their colonization efforts by bringing Islam to the continent. \\n\\nAccuracy: INCORRECT. The text states that European powers \"justified their colonization efforts by claiming to bring civilization, Christianity, and economic development to the continent.\" Islam is not mentioned as one of the claims made by European powers.\n",
        "      Final Answer: INCORRECT\n",
        "\n",
        "      <CONTEXT>\n",
        "      {context}\n",
        "      </CONTEXT>\n",
        "      <RESPONSE>\n",
        "      {sentence}\n",
        "      </RESPONSE>\n",
        "      Evaluation:\n",
        "      \"\"\"\n",
        "      evaluation = self.try_n_times(correctness, lambda x: x.split(\"\\n\")[-1].split(\" \")[-1].strip() == \"CORRECT\", 3)\n",
        "      if not evaluation:\n",
        "        incorrect_sentences.append(sentence)\n",
        "      correct = correct and evaluation\n",
        "    prompt = f\"\"\"Does the response answer the question? Remember that a response with incorrect information can still answer the question if the misinformation does not overly impact the main points of the response. Return one word YES or NO:\n",
        "    Example1:\n",
        "    <QUESTION>\n",
        "    </QUESTION>\n",
        "    <RESPONSE>\n",
        "    </RESPONSE>\n",
        "    Answer: YES\n",
        "\n",
        "    <QUESTION>\n",
        "    {q}\n",
        "    </QUESTION>\n",
        "    <RESPONSE>\n",
        "    {student_response}\n",
        "    </RESPONSE>\n",
        "    \"\"\"\n",
        "    yesno = self.try_n_times(prompt, lambda x: x.split(\"\\n\")[-1].split(\" \")[-1].strip() == \"YES\", 2)\n",
        "    return (correct, incorrect_sentences, yesno)\n",
        "\n",
        "  def evaluate_core_standard(self, context, student_response):\n",
        "    prompt = f\"\"\"\n",
        "    {self.rubric}\n",
        "    Score the following response from 1-3. Remember the student can only\n",
        "    use information provided in the context and scale the score accordingly. Explain why step by step.\n",
        "    Then, unless the score is 3, give suggestions on how to improve the score.\n",
        "    <CONTEXT>\n",
        "    {context}\n",
        "    </CONTEXT>\n",
        "    <RESPONSE>\n",
        "    {student_response}\n",
        "    </RESPONSE>\n",
        "    \"\"\"\n",
        "    feedback = self.t_chain.predict(task=prompt)\n",
        "    return feedback\n",
        "\n",
        "  def evaluate_response(self, context, question, student_response, max_edits=2):\n",
        "    correct, incorrect_sentences, answered_question = self.evaluate_correctness(context, question, student_response)\n",
        "    standard_feedback = self.evaluate_core_standard(context, student_response)\n",
        "    if correct and answered_question:\n",
        "      feedback = \"Good Job! You answered the question correctly.\\n\"\n",
        "    elif correct and not answered_question:\n",
        "      feedback = \"Your response was accurate, but did not adequately answer the question.\\n\"\n",
        "      prompt = f\"Explain step by step why the response does not adequately answer the question: {question}\"\n",
        "      feedback += self.t_chain.predict(task=prompt) + \"\\n\"\n",
        "    elif not correct and answered_question:\n",
        "      feedback = \"Your response contained incorrect information but overall still answered the question.\\n\"\n",
        "      for sentence in incorrect_sentences:\n",
        "        prompt = f\"Explain step by step why the sentence '{sentence}' is incorrect given the provided context\"\n",
        "        feedback += self.t_chain.predict(task=prompt) + \"\\n\"\n",
        "    else:\n",
        "      feedback = \"Your response contained incorrect information and did not adequately answer the question.\\n\"\n",
        "      for sentence in incorrect_sentences:\n",
        "        prompt = f\"Explain step by step why the sentence '{sentence}' is incorrect given the provided context\"\n",
        "        feedback += self.t_chain.predict(task=prompt) + \"\\n\"\n",
        "      prompt = f\"Explain step by step why the response does not adequately answer the question: {question}\"\n",
        "      feedback += self.t_chain.predict(task=prompt) + \"\\n\"\n",
        "    feedback = feedback + \"\\n\\n\" + standard_feedback\n",
        "\n",
        "    passed_qa = False\n",
        "    num_edits = 0\n",
        "    while not passed_qa and num_edits <= max_edits and self.enabled_qa:\n",
        "      passed_qa, failed_check = self.check_feedback_quality(feedback)\n",
        "      if not passed_qa:\n",
        "        prompt = f\"\"\"Return an edited version of this feedback so that it does not {failed_check}.\n",
        "        <FEEDBACK>\n",
        "        {feedback}\n",
        "        </FEEDBACK>\n",
        "        \"\"\"\n",
        "        print(f\"Improving feedback so that it does not {failed_check}\")\n",
        "        num_edits += 1\n",
        "        feedback = self.quality_chain.predict(task=prompt)\n",
        "    return feedback\n",
        "\n",
        "TA = TeachingStaff(OPENAI_API_KEY, standard=\"Draw evidence from literary or informational texts to support analysis, reflection, and research.\", enable_qa=True)\n",
        "TA.set_topic(\"Dinosaur extinction event\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pokxzi_lvv_"
      },
      "outputs": [],
      "source": [
        "frq, question = TA.generate_FRQ()\n",
        "print(frq + \"\\n\\n\" + question)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "student_response = TA.generate_response(frq + \"\\n\\n\" + question)\n",
        "print(student_response)"
      ],
      "metadata": {
        "id": "vhRu9jZPI2LC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "student_response = \"\"\"The evidence found in geological deposits that supports the widely accepted theory of the cause of the Cretaceous-Paleogene extinction event is a distinct layer of clay enriched with the rare element iridium. This element is usually found in higher concentrations in asteroids and comets than on Earth. This iridium layer is found in many different geological deposits all over the world and is dated right back to the time of the extinction event, which makes it a strong evidence for the asteroid or comet impact theory. This evidence connects to the survival of certain animal groups in a very interesting way. While the fallout from the asteroid or comet impact was devastating, it didn't wipe out all the life on Earth. The theory is that these animals were able to adapt to the harsh conditions following the impact, like the \"nuclear winter\" effect caused by dust and debris blocking out sunlight. The T-Rex also survived. The mammals and birds that managed to survive this tough time were able to thrive and evolve in the new environment, eventually becoming the dominant species on Earth.\"\"\"\n",
        "feedback = TA.evaluate_response(frq, question, student_response)\n",
        "print(feedback)"
      ],
      "metadata": {
        "id": "n-zL1kT-I3Xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deploy Web Demo"
      ],
      "metadata": {
        "id": "_jFsWp5eLzO7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QaSv8wwmn2d"
      },
      "outputs": [],
      "source": [
        " %%writefile app.py\n",
        "import langchain\n",
        "import os, streamlit as st\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.cache import InMemoryCache\n",
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    PromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "\n",
        "frq_quality_rubric = [\"Can the question be answered using only evidence from the context above?\",\n",
        "                      \"Does the question test understanding of the information presented in the context?\"]\n",
        "feedback_quality_rubric = [\"contradict itself\",\n",
        "                           \"repeat the same points\"]\n",
        "\n",
        "class TeachingStaff:\n",
        "\n",
        "  def __init__(self, api_key, standard, topic=None, rubric=None, enable_qa=False):\n",
        "    chatllm = ChatOpenAI(openai_api_key=api_key, model_name='gpt-4') #model_name='gpt-4'\n",
        "    self.llm = OpenAI(openai_api_key=api_key, model_name='gpt-4') #model_name='gpt-4'\n",
        "\n",
        "    if topic != None:\n",
        "      self.set_topic(topic)\n",
        "    self.standard = standard\n",
        "\n",
        "    self.knowledge_gen_chain, self.question_asking_chain, \\\n",
        "    self.t_chain, self.s_chain, self.quality_chain = self.initialize_agents(chatllm)\n",
        "\n",
        "    if rubric != None:\n",
        "      self.rubric = rubric\n",
        "    else:\n",
        "      self.rubric = self.generate_rubric()\n",
        "\n",
        "    self.enabled_qa = enable_qa\n",
        "\n",
        "  def set_topic(self, topic):\n",
        "    self.topic = topic\n",
        "\n",
        "  def get_rubric(self):\n",
        "    return self.rubric\n",
        "\n",
        "  def initialize_agents(self, llm):\n",
        "    # knowledge generation chain\n",
        "    kg_template = \"\"\"Mary is an super-intelligent, advanced AI task executor that posseses accurate knowledge on every topic.\n",
        "     It does not mention itself or admit its nature as an AI.\n",
        "     It uses the voice of a primary or secondary source.\n",
        "     It does not use first or second person except when quoting a source.\n",
        "     It fulfills requests exactly and concisely\n",
        "     For the following requests, you will respond and do tasks as Mary.\n",
        "\n",
        "     {chat_history}\n",
        "     Human: {task}\n",
        "     Mary:\"\"\"\n",
        "    kg_prompt = PromptTemplate(input_variables=[\"chat_history\", \"task\"], template=kg_template)\n",
        "    kg_memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "    kg_chain = LLMChain(llm=llm, prompt=kg_prompt, verbose=False, memory=kg_memory)\n",
        "\n",
        "    # question asking chain\n",
        "    qa_template = f\"\"\"John is an super-intelligent question asking AI with critical reading and thinking skills.\n",
        "     It does not mention itself or admit its nature as an AI.\n",
        "     It does not use first or second person except when quoting a source.\n",
        "     It specializes in generating insightful questions that test understanding of a text passage.\n",
        "     Its questions expect the response to exhibit the standard: {self.standard}.\n",
        "     For the following requests, you will respond and do tasks as John.\\n\\n\"\"\" + \\\n",
        "     \"\"\"\n",
        "     {chat_history}\n",
        "     Human: {task}\n",
        "     John:\"\"\"\n",
        "    qa_prompt = PromptTemplate(input_variables=[\"chat_history\", \"task\"], template=qa_template)\n",
        "    qa_memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "    qa_chain = LLMChain(llm=llm, prompt=qa_prompt, verbose=False, memory=qa_memory)\n",
        "\n",
        "    # teaching chain\n",
        "    t_template = \"\"\"Susan is an super-intelligent, advanced educational AI that is an expert at teaching students.\n",
        "     It is knowledgable in all common student mistakes.\n",
        "     It does not mention itself or admit its nature as an AI.\n",
        "     It does not use first or second person except when quoting a source.\n",
        "     It fulfills requests exactly and concisely\n",
        "     It is extremely competent in evaluating free response questions, identifying false information,\n",
        "     and providing the best advice for students to improve their writing with respect to a rubric.\n",
        "     For the following requests, you will respond and do tasks as Susan.\n",
        "\n",
        "     {chat_history}\n",
        "     Human: {task}\n",
        "     Susan:\"\"\"\n",
        "    t_prompt = PromptTemplate(input_variables=[\"chat_history\", \"task\"], template=t_template)\n",
        "    t_memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "    t_chain = LLMChain(llm=llm, prompt=t_prompt, verbose=False, memory=t_memory)\n",
        "\n",
        "    # student chain (simulate student for testing)\n",
        "    s_template = \"\"\"Zach is a human 4th grade student that is doing a writing assignment.\n",
        "    For the following requests, you will respond and write as Zach.\n",
        "\n",
        "    {chat_history}\n",
        "    {task}\n",
        "    Zach:\"\"\"\n",
        "    s_prompt = PromptTemplate(input_variables=[\"chat_history\", \"task\"], template=s_template)\n",
        "    s_memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "    s_chain = LLMChain(llm=llm, prompt=s_prompt, verbose=False, memory=s_memory)\n",
        "\n",
        "    # quality assurance agent\n",
        "    quality_template = \"\"\"Jan is an advanced teaching quality assurance AI that is an expert at\n",
        "    editing educational text to promote concise, helpful, and easy to understand material for students.\n",
        "    It does not mention itself or admit its nature as an AI.\n",
        "    It fulfills requests exactly and concisely without repeating the request.\n",
        "    It does not use first or second person except when quoting a source.\n",
        "    For the folling requests, you will respond as Jan.\n",
        "\n",
        "    {chat_history}\n",
        "    {task}\n",
        "    Jan:\"\"\"\n",
        "    quality_prompt = PromptTemplate(input_variables=[\"chat_history\", \"task\"], template=quality_template)\n",
        "    quality_memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "    quality_chain = LLMChain(llm=llm, prompt=quality_prompt, verbose=False, memory=quality_memory)\n",
        "\n",
        "    return kg_chain, qa_chain, t_chain, s_chain, quality_chain\n",
        "\n",
        "  def generate_FRQ(self, max_retry=2):\n",
        "    intro, context = self.generate_intro_context()\n",
        "    question = self.generate_question(context)\n",
        "    passed_qa = False\n",
        "    tries = 0\n",
        "    while(not passed_qa and self.enabled_qa and tries <= max_retry):\n",
        "      passed_qa, failed_check = self.check_FRQ_quality(intro + \"\\n\" + context, question)\n",
        "      if not passed_qa:\n",
        "        print(\"discarded question: \" + question + \"\\n due to failing to pass: \" + failed_check + \"\\n\")\n",
        "        question = self.generate_question(context)\n",
        "        tries = tries + 1\n",
        "\n",
        "    return intro + \"\\n\" + context, question\n",
        "\n",
        "  def check_FRQ_quality(self, context, question):\n",
        "    prompt = f\"\"\"Refer to the following context and question for the following requests:\n",
        "    <CONTEXT>\n",
        "    {context}\n",
        "    </CONTEXT>\n",
        "    <QUESTION>\n",
        "    {question}\n",
        "    </QUESTION>\n",
        "    \"\"\"\n",
        "    self.quality_chain.predict(task=prompt)\n",
        "\n",
        "    for check in frq_quality_rubric:\n",
        "      if (\"YES\" not in self.quality_chain.predict(task=check + \" Answer with one word YES or NO.\")):\n",
        "        return False, check\n",
        "    return True, \"\"\n",
        "\n",
        "  def check_feedback_quality(self, feedback):\n",
        "    prompt = f\"\"\"Refer to the following feedback for the following requests:\n",
        "    <FEEDBACK>\n",
        "    {feedback}\n",
        "    </FEEDBACK>\n",
        "    \"\"\"\n",
        "    self.quality_chain.predict(task=prompt)\n",
        "\n",
        "    for check in feedback_quality_rubric:\n",
        "      if (\"YES\" in self.quality_chain.predict(task=\"Does the feedback \" + check + \"? Answer with one word YES or NO.\")):\n",
        "        return False, check\n",
        "    return True, \"\"\n",
        "\n",
        "  def generate_intro_context(self):\n",
        "    intro_prompt = f\"Write a short 2 sentence introduction for the topic '{self.topic}'\"\n",
        "    intro = self.knowledge_gen_chain.predict(task=intro_prompt)\n",
        "    knowledge_prompt = f\"generate 8 facts related to the topic '{self.topic}' which could follow your introduction\"\n",
        "    knowledge = self.knowledge_gen_chain.predict(task=knowledge_prompt)\n",
        "    context_prompt = f\"generate 3 paragraphs about the topic naturally utilizing the above facts incorporating quotes if necessary\"\n",
        "    context = self.knowledge_gen_chain.predict(task=context_prompt)\n",
        "    return intro, context\n",
        "\n",
        "  def generate_question(self, context):\n",
        "    prompt = f\"Generate an open-ended question which can be answered solely by drawing evidence from the context:\\n'{context}'\"\n",
        "    question = self.question_asking_chain.predict(task=prompt)\n",
        "    return question\n",
        "\n",
        "  def generate_rubric(self):\n",
        "    prompt = \\\n",
        "    f\"\"\"Concisely generate a rubric for grading an essay answer based on how well it demonstrates the core standard '{self.standard}'.\n",
        "    It should assign a score from 1-3 and give criteria for meeting each score cutoff. \"\"\"\n",
        "    return self.t_chain.predict(task=prompt)\n",
        "\n",
        "  def generate_response(self, frq):\n",
        "    prompt = f\"Answer the following question in paragraph form:\\n {frq}\"\n",
        "    return self.s_chain.predict(task=prompt)\n",
        "\n",
        "  def try_n_times(self, prompt, condition, n):\n",
        "    for i in range(0, n):\n",
        "      if condition(self.llm(prompt)):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "  def evaluate_correctness(self, context, q, student_response):\n",
        "    sentences = sent_tokenize(student_response.strip())\n",
        "    correct = True\n",
        "    incorrect_sentences = []\n",
        "    for sentence in sentences:\n",
        "      correctness = f\"\"\"Identify claims from the response and and evaluate accuracy of each using evidence from the context step by step. Evidence from the context is not needed if the claim is common sense. Return a final answer of CORRECT if all claims are accurate, and INCORRECT otherwise..\n",
        "      Example1:\n",
        "      <CONTEXT>\n",
        "      </CONTEXT>\n",
        "      <RESPONSE>\n",
        "      During the Scramble for Africa, European powers justified their colonization efforts by claiming to bring civilization, Islam, and economic development to the continent.\n",
        "      </RESPONSE>\n",
        "      Evaluation:\n",
        "      Claim 1: European powers justified their colonization efforts by bringing civilization to the continent. \\n\\nAccuracy: CORRECT. The text states that European powers \"justified their colonization efforts by claiming to bring civilization, Christianity, and economic development to the continent.\" \\n\\nClaim 2: European powers justified their colonization efforts by bringing Islam to the continent. \\n\\nAccuracy: INCORRECT. The text states that European powers \"justified their colonization efforts by claiming to bring civilization, Christianity, and economic development to the continent.\" Islam is not mentioned as one of the claims made by European powers.\n",
        "      Final Answer: INCORRECT\n",
        "\n",
        "      <CONTEXT>\n",
        "      {context}\n",
        "      </CONTEXT>\n",
        "      <RESPONSE>\n",
        "      {sentence}\n",
        "      </RESPONSE>\n",
        "      Evaluation:\n",
        "      \"\"\"\n",
        "      evaluation = self.try_n_times(correctness, lambda x: x.split(\"\\n\")[-1].split(\" \")[-1].strip() == \"CORRECT\", 3)\n",
        "      if not evaluation:\n",
        "        incorrect_sentences.append(sentence)\n",
        "      correct = correct and evaluation\n",
        "    prompt = f\"\"\"Does the response answer the question? Remember that a response with incorrect information can still answer the question if the misinformation does not overly impact the main points of the response. Return one word YES or NO:\n",
        "    Example1:\n",
        "    <QUESTION>\n",
        "    </QUESTION>\n",
        "    <RESPONSE>\n",
        "    </RESPONSE>\n",
        "    Answer: YES\n",
        "\n",
        "    <QUESTION>\n",
        "    {q}\n",
        "    </QUESTION>\n",
        "    <RESPONSE>\n",
        "    {student_response}\n",
        "    </RESPONSE>\n",
        "    \"\"\"\n",
        "    yesno = self.try_n_times(prompt, lambda x: x.split(\"\\n\")[-1].split(\" \")[-1].strip() == \"YES\", 2)\n",
        "    return (correct, incorrect_sentences, yesno)\n",
        "\n",
        "  def evaluate_core_standard(self, context, student_response):\n",
        "    prompt = f\"\"\"\n",
        "    {self.rubric}\n",
        "    Score the following response from 1-3. Remember the student can only\n",
        "    use information provided in the context and scale the score accordingly. Explain why step by step.\n",
        "    Then, unless the score is 3, give suggestions on how to improve the score.\n",
        "    <CONTEXT>\n",
        "    {context}\n",
        "    </CONTEXT>\n",
        "    <RESPONSE>\n",
        "    {student_response}\n",
        "    </RESPONSE>\n",
        "    \"\"\"\n",
        "    feedback = self.t_chain.predict(task=prompt)\n",
        "    return feedback\n",
        "\n",
        "  def evaluate_response(self, context, question, student_response, max_edits=2):\n",
        "    correct, incorrect_sentences, answered_question = self.evaluate_correctness(context, question, student_response)\n",
        "    standard_feedback = self.evaluate_core_standard(context, student_response)\n",
        "    if correct and answered_question:\n",
        "      feedback = \"Good Job! You answered the question correctly.\\n\"\n",
        "    elif correct and not answered_question:\n",
        "      feedback = \"Your response was accurate, but did not adequately answer the question.\\n\"\n",
        "      prompt = f\"Explain step by step why the response does not adequately answer the question: {question}\"\n",
        "      feedback += self.t_chain.predict(task=prompt) + \"\\n\"\n",
        "    elif not correct and answered_question:\n",
        "      feedback = \"Your response contained incorrect information but overall still answered the question.\\n\"\n",
        "      for sentence in incorrect_sentences:\n",
        "        prompt = f\"Explain step by step why the sentence '{sentence}' is incorrect given the provided context\"\n",
        "        feedback += self.t_chain.predict(task=prompt) + \"\\n\"\n",
        "    else:\n",
        "      feedback = \"Your response contained incorrect information and did not adequately answer the question.\\n\"\n",
        "      for sentence in incorrect_sentences:\n",
        "        prompt = f\"Explain step by step why the sentence '{sentence}' is incorrect given the provided context\"\n",
        "        feedback += self.t_chain.predict(task=prompt) + \"\\n\"\n",
        "      prompt = f\"Explain step by step why the response does not adequately answer the question: {question}\"\n",
        "      feedback += self.t_chain.predict(task=prompt) + \"\\n\"\n",
        "    feedback = feedback + \"\\n\\n\" + standard_feedback\n",
        "\n",
        "    passed_qa = False\n",
        "    num_edits = 0\n",
        "    while not passed_qa and num_edits <= max_edits and self.enabled_qa:\n",
        "      passed_qa, failed_check = self.check_feedback_quality(feedback)\n",
        "      if not passed_qa:\n",
        "        prompt = f\"\"\"Return an edited version of this feedback so that it does not {failed_check}.\n",
        "        <FEEDBACK>\n",
        "        {feedback}\n",
        "        </FEEDBACK>\n",
        "        \"\"\"\n",
        "        print(f\"Improving feedback so that it does not {failed_check}\")\n",
        "        num_edits += 1\n",
        "        feedback = self.quality_chain.predict(task=prompt)\n",
        "    return feedback\n",
        "\n",
        "st.title(\"Automated Teaching Assistant\")\n",
        "\n",
        "OPENAI_API_KEY = st.text_input(\"OpenAI API Key\", type=\"password\")\n",
        "topic = st.text_input(\"Choose a learning topic\")\n",
        "core_standard = st.text_input(\"Provide a core learning standard\")\n",
        "\n",
        "if st.button(\"Generate Free Response Question About Topic\"):\n",
        "  if not OPENAI_API_KEY.strip() or not topic.strip() or not core_standard.strip():\n",
        "        st.write(f\"Please complete the missing fields.\")\n",
        "  else:\n",
        "      try:\n",
        "        TS = TeachingStaff(OPENAI_API_KEY, standard=core_standard, enable_qa=True)\n",
        "        TS.set_topic(topic)\n",
        "        context, question = TS.generate_FRQ()\n",
        "\n",
        "        st.session_state['TS'] = TS\n",
        "        st.session_state['context'] = context\n",
        "        st.session_state['question'] = question\n",
        "        st.session_state['rubric'] = TS.get_rubric()\n",
        "      except Exception as e:\n",
        "          st.write(f\"An error occurred: {e}\")\n",
        "try:\n",
        "  st.write(st.session_state['context'])\n",
        "  st.write(st.session_state['question'])\n",
        "  st.write(\"The following rubric will be used to evaluate your answer:\")\n",
        "  st.write(st.session_state['rubric'])\n",
        "except:\n",
        "  pass\n",
        "\n",
        "if st.button(\"Use ChatGPT to generate a model answer!\"):\n",
        "  try:\n",
        "    AI_response = st.session_state['TS'].generate_response(st.session_state['context'] + \"\\n\\n\" + st.session_state['question'])\n",
        "    st.session_state['AI_response'] = AI_response\n",
        "  except Exception as e:\n",
        "    st.write(f\"An error occurred: {e}\")\n",
        "\n",
        "try:\n",
        "  st.write(st.session_state['AI_response'])\n",
        "except:\n",
        "  pass\n",
        "\n",
        "student_response = st.text_area(\"Answer here\", height=200)\n",
        "if st.button(\"Evaluate my answer!\"):\n",
        "  if not student_response.strip():\n",
        "    st.write(f\"Please complete the missing fields.\")\n",
        "  else:\n",
        "    try:\n",
        "      feedback = st.session_state['TS'].evaluate_response(st.session_state['context'], st.session_state['question'], student_response)\n",
        "      st.session_state['feedback'] = feedback\n",
        "    except Exception as e:\n",
        "      st.write(f\"An error occurred: {e}\")\n",
        "try:\n",
        "  st.write(st.session_state['feedback'])\n",
        "except:\n",
        "  pass\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"import urllib\n",
        "# Paste returned IP address into the field on the url generated by local tunnel to access the web demo\n",
        "print(\"Password/Enpoint IP for localtunnel is:\",urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))"
      ],
      "metadata": {
        "id": "2f1uxoMIZcXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/content/logs.txt &"
      ],
      "metadata": {
        "id": "qaQM41rwMJw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "mHXbuKhmMdhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gUS02Sa0VSnL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9JaqmvzzfNelQnAY1gVdg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}