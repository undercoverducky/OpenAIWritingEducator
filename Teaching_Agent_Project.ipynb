{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/undercoverducky/OpenAIWritingEducator/blob/main/Teaching_Agent_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Prerequisites\n"
      ],
      "metadata": {
        "id": "Z0FlAqpfuRem"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52qPTwHyinGv",
        "outputId": "7d711670-bb92-46ea-b29a-84237eccedb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.253-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.19)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.11 (from langchain)\n",
            "  Downloading langsmith-0.0.19-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.12)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, openapi-schema-pydantic, langsmith, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.5.14 langchain-0.0.253 langsmith-0.0.19 marshmallow-3.20.1 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 typing-inspect-0.9.0\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.8\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.25.0-py2.py3-none-any.whl (8.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.1)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.6)\n",
            "Requirement already satisfied: importlib-metadata<7,>=1.4 in /usr/lib/python3/dist-packages (from streamlit) (4.6.4)\n",
            "Requirement already satisfied: numpy<2,>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.22.4)\n",
            "Requirement already satisfied: packaging<24,>=16.8 in /usr/local/lib/python3.10/dist-packages (from streamlit) (23.1)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.5.3)\n",
            "Requirement already satisfied: pillow<10,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=6.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n",
            "Collecting pympler<2,>=0.9 (from streamlit)\n",
            "  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3,>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.18 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.27.1)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.4.2)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.2.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.7.1)\n",
            "Collecting tzlocal<5,>=1.1 (from streamlit)\n",
            "  Downloading tzlocal-4.3.1-py3-none-any.whl (20 kB)\n",
            "Collecting validators<1,>=0.2 (from streamlit)\n",
            "  Downloading validators-0.20.0.tar.gz (30 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading GitPython-3.1.32-py3-none-any.whl (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck<1,>=0.8 (from streamlit)\n",
            "  Downloading pydeck-0.8.0-py2.py3-none-any.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.1)\n",
            "Collecting watchdog>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.2)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.3.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3,>=2.7.3->streamlit) (1.16.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.18->streamlit) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.18->streamlit) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.18->streamlit) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.18->streamlit) (3.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.14.0)\n",
            "Collecting pytz-deprecation-shim (from tzlocal<5,>=1.1->streamlit)\n",
            "  Downloading pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from validators<1,>=0.2->streamlit) (4.4.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.1.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.19.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Collecting tzdata (from pytz-deprecation-shim->tzlocal<5,>=1.1->streamlit)\n",
            "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: validators\n",
            "  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19580 sha256=ce51f1bdb0268559e11a9e8bdfcc6f49c06f99ee38d0f8d6c04f4364615ceaa2\n",
            "  Stored in directory: /root/.cache/pip/wheels/f2/ed/dd/d3a556ad245ef9dc570c6bcd2f22886d17b0b408dd3bbb9ac3\n",
            "Successfully built validators\n",
            "Installing collected packages: watchdog, validators, tzdata, smmap, pympler, pytz-deprecation-shim, pydeck, gitdb, tzlocal, gitpython, streamlit\n",
            "  Attempting uninstall: tzlocal\n",
            "    Found existing installation: tzlocal 5.0.1\n",
            "    Uninstalling tzlocal-5.0.1:\n",
            "      Successfully uninstalled tzlocal-5.0.1\n",
            "Successfully installed gitdb-4.0.10 gitpython-3.1.32 pydeck-0.8.0 pympler-1.0.1 pytz-deprecation-shim-0.1.0.post0 smmap-5.0.0 streamlit-1.25.0 tzdata-2023.3 tzlocal-4.3.1 validators-0.20.0 watchdog-3.0.0\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.4.0\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[34;40mnotice\u001b[0m\u001b[35m\u001b[0m created a lockfile as package-lock.json. You should commit this file.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "+ localtunnel@2.0.2\n",
            "added 22 packages from 22 contributors and audited 22 packages in 2.491s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found \u001b[92m0\u001b[0m vulnerabilities\n",
            "\n",
            "\u001b[K\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install streamlit\n",
        "!pip install tiktoken\n",
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prototype Source code and Testing\n"
      ],
      "metadata": {
        "id": "ZUtYON4oua8O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQIqX52am5LA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ae16cd7-89b7-4a0a-ff6c-20df010e5e23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/llms/openai.py:201: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/llms/openai.py:786: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import langchain\n",
        "import os, streamlit as st\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.cache import InMemoryCache\n",
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    PromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "\n",
        "frq_quality_rubric = [\"Can the question be answered using only evidence from the context above?\",\n",
        "                      \"Does the question test understanding of the information presented in the context?\"]\n",
        "feedback_quality_rubric = [\"contradict itself\",\n",
        "                           \"repeat the same points\"]\n",
        "\n",
        "class TeachingStaff:\n",
        "\n",
        "  def __init__(self, api_key, standard, topic=None, rubric=None, enable_qa=False):\n",
        "    chatllm = ChatOpenAI(openai_api_key=api_key, model_name='gpt-4') #model_name='gpt-4'\n",
        "    self.llm = OpenAI(openai_api_key=api_key, model_name='gpt-4') #model_name='gpt-4'\n",
        "\n",
        "    if topic != None:\n",
        "      self.set_topic(topic)\n",
        "    self.standard = standard\n",
        "\n",
        "    self.knowledge_gen_chain, self.question_asking_chain, \\\n",
        "    self.t_chain, self.s_chain, self.quality_chain = self.initialize_agents(chatllm)\n",
        "\n",
        "    if rubric != None:\n",
        "      self.rubric = rubric\n",
        "    else:\n",
        "      self.rubric = self.generate_rubric()\n",
        "\n",
        "    self.enabled_qa = enable_qa\n",
        "\n",
        "  def set_topic(self, topic):\n",
        "    self.topic = topic\n",
        "\n",
        "  def get_rubric(self):\n",
        "    return self.rubric\n",
        "\n",
        "  def initialize_agents(self, llm):\n",
        "    # knowledge generation chain\n",
        "    kg_template = \"\"\"Mary is an super-intelligent, advanced AI task executor that posseses accurate knowledge on every topic.\n",
        "     It does not mention itself or admit its nature as an AI.\n",
        "     It uses the voice of a primary or secondary source.\n",
        "     It does not use first or second person except when quoting a source.\n",
        "     It fulfills requests exactly and concisely\n",
        "     For the following requests, you will respond and do tasks as Mary.\n",
        "\n",
        "     {chat_history}\n",
        "     Human: {task}\n",
        "     Mary:\"\"\"\n",
        "    kg_prompt = PromptTemplate(input_variables=[\"chat_history\", \"task\"], template=kg_template)\n",
        "    kg_memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "    kg_chain = LLMChain(llm=llm, prompt=kg_prompt, verbose=False, memory=kg_memory)\n",
        "\n",
        "    # question asking chain\n",
        "    qa_template = f\"\"\"John is an super-intelligent question asking AI with critical reading and thinking skills.\n",
        "     It does not mention itself or admit its nature as an AI.\n",
        "     It does not use first or second person except when quoting a source.\n",
        "     It specializes in generating insightful questions that test understanding of a text passage.\n",
        "     Its questions expect the response to exhibit the standard: {self.standard}.\n",
        "     For the following requests, you will respond and do tasks as John.\\n\\n\"\"\" + \\\n",
        "     \"\"\"\n",
        "     {chat_history}\n",
        "     Human: {task}\n",
        "     John:\"\"\"\n",
        "    qa_prompt = PromptTemplate(input_variables=[\"chat_history\", \"task\"], template=qa_template)\n",
        "    qa_memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "    qa_chain = LLMChain(llm=llm, prompt=qa_prompt, verbose=False, memory=qa_memory)\n",
        "\n",
        "    # teaching chain\n",
        "    t_template = \"\"\"Susan is an super-intelligent, advanced educational AI that is an expert at teaching students.\n",
        "     It is knowledgable in all common student mistakes.\n",
        "     It does not mention itself or admit its nature as an AI.\n",
        "     It does not use first or second person except when quoting a source.\n",
        "     It fulfills requests exactly and concisely\n",
        "     It is extremely competent in evaluating free response questions, identifying false information,\n",
        "     and providing the best advice for students to improve their writing with respect to a rubric.\n",
        "     For the following requests, you will respond and do tasks as Susan.\n",
        "\n",
        "     {chat_history}\n",
        "     Human: {task}\n",
        "     Susan:\"\"\"\n",
        "    t_prompt = PromptTemplate(input_variables=[\"chat_history\", \"task\"], template=t_template)\n",
        "    t_memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "    t_chain = LLMChain(llm=llm, prompt=t_prompt, verbose=False, memory=t_memory)\n",
        "\n",
        "    # student chain (simulate student for testing)\n",
        "    s_template = \"\"\"Zach is a human 4th grade student that is doing a writing assignment.\n",
        "    For the following requests, you will respond and write as Zach.\n",
        "\n",
        "    {chat_history}\n",
        "    {task}\n",
        "    Zach:\"\"\"\n",
        "    s_prompt = PromptTemplate(input_variables=[\"chat_history\", \"task\"], template=s_template)\n",
        "    s_memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "    s_chain = LLMChain(llm=llm, prompt=s_prompt, verbose=False, memory=s_memory)\n",
        "\n",
        "    # quality assurance agent\n",
        "    quality_template = \"\"\"Jan is an advanced teaching quality assurance AI that is an expert at\n",
        "    editing educational text to promote concise, helpful, and easy to understand material for students.\n",
        "    It does not mention itself or admit its nature as an AI.\n",
        "    It fulfills requests exactly and concisely without repeating the request.\n",
        "    It does not use first or second person except when quoting a source.\n",
        "    For the folling requests, you will respond as Jan.\n",
        "\n",
        "    {chat_history}\n",
        "    {task}\n",
        "    Jan:\"\"\"\n",
        "    quality_prompt = PromptTemplate(input_variables=[\"chat_history\", \"task\"], template=quality_template)\n",
        "    quality_memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "    quality_chain = LLMChain(llm=llm, prompt=quality_prompt, verbose=False, memory=quality_memory)\n",
        "\n",
        "    return kg_chain, qa_chain, t_chain, s_chain, quality_chain\n",
        "\n",
        "  def generate_FRQ(self, max_retry=2):\n",
        "    intro, context = self.generate_intro_context()\n",
        "    question = self.generate_question(context)\n",
        "    passed_qa = False\n",
        "    tries = 0\n",
        "    while(not passed_qa and self.enabled_qa and tries <= max_retry):\n",
        "      passed_qa, failed_check = self.check_FRQ_quality(intro + \"\\n\" + context, question)\n",
        "      if not passed_qa:\n",
        "        print(\"discarded question: \" + question + \"\\n due to failing to pass: \" + failed_check + \"\\n\")\n",
        "        question = self.generate_question(context)\n",
        "        tries = tries + 1\n",
        "\n",
        "    return intro + \"\\n\" + context, question\n",
        "\n",
        "  def check_FRQ_quality(self, context, question):\n",
        "    prompt = f\"\"\"Refer to the following context and question for the following requests:\n",
        "    <CONTEXT>\n",
        "    {context}\n",
        "    </CONTEXT>\n",
        "    <QUESTION>\n",
        "    {question}\n",
        "    </QUESTION>\n",
        "    \"\"\"\n",
        "    self.quality_chain.predict(task=prompt)\n",
        "\n",
        "    for check in frq_quality_rubric:\n",
        "      if (\"YES\" not in self.quality_chain.predict(task=check + \" Answer with one word YES or NO.\")):\n",
        "        return False, check\n",
        "    return True, \"\"\n",
        "\n",
        "  def check_feedback_quality(self, feedback):\n",
        "    prompt = f\"\"\"Refer to the following feedback for the following requests:\n",
        "    <FEEDBACK>\n",
        "    {feedback}\n",
        "    </FEEDBACK>\n",
        "    \"\"\"\n",
        "    self.quality_chain.predict(task=prompt)\n",
        "\n",
        "    for check in feedback_quality_rubric:\n",
        "      if (\"YES\" in self.quality_chain.predict(task=\"Does the feedback \" + check + \"? Answer with one word YES or NO.\")):\n",
        "        return False, check\n",
        "    return True, \"\"\n",
        "\n",
        "  def generate_intro_context(self):\n",
        "    intro_prompt = f\"Write a short 2 sentence introduction for the topic '{self.topic}'\"\n",
        "    intro = self.knowledge_gen_chain.predict(task=intro_prompt)\n",
        "    knowledge_prompt = f\"generate 8 facts related to the topic '{self.topic}' which could follow your introduction\"\n",
        "    knowledge = self.knowledge_gen_chain.predict(task=knowledge_prompt)\n",
        "    context_prompt = f\"generate 3 paragraphs about the topic naturally utilizing the above facts incorporating quotes if necessary\"\n",
        "    context = self.knowledge_gen_chain.predict(task=context_prompt)\n",
        "    return intro, context\n",
        "\n",
        "  def generate_question(self, context):\n",
        "    prompt = f\"Generate an open-ended question which can be answered solely by drawing evidence from the context:\\n'{context}'\"\n",
        "    question = self.question_asking_chain.predict(task=prompt)\n",
        "    return question\n",
        "\n",
        "  def generate_rubric(self):\n",
        "    prompt = \\\n",
        "    f\"\"\"Concisely generate a rubric for grading an essay answer based on how well it demonstrates the core standard '{self.standard}'.\n",
        "    It should assign a score from 1-3 and give criteria for meeting each score cutoff. \"\"\"\n",
        "    return self.t_chain.predict(task=prompt)\n",
        "\n",
        "  def generate_response(self, frq):\n",
        "    prompt = f\"Answer the following question in paragraph form:\\n {frq}\"\n",
        "    return self.s_chain.predict(task=prompt)\n",
        "\n",
        "  def try_n_times(self, prompt, condition, n):\n",
        "    for i in range(0, n):\n",
        "      if condition(self.llm(prompt)):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "  def evaluate_correctness(self, context, q, student_response):\n",
        "    sentences = sent_tokenize(student_response.strip())\n",
        "    correct = True\n",
        "    incorrect_sentences = []\n",
        "    for sentence in sentences:\n",
        "      correctness = f\"\"\"Identify claims from the response and and evaluate accuracy of each using evidence from the context step by step. Evidence from the context is not needed if the claim is common sense. Return a final answer of CORRECT if all claims are accurate, and INCORRECT otherwise..\n",
        "      Example1:\n",
        "      <CONTEXT>\n",
        "      </CONTEXT>\n",
        "      <RESPONSE>\n",
        "      During the Scramble for Africa, European powers justified their colonization efforts by claiming to bring civilization, Islam, and economic development to the continent.\n",
        "      </RESPONSE>\n",
        "      Evaluation:\n",
        "      Claim 1: European powers justified their colonization efforts by bringing civilization to the continent. \\n\\nAccuracy: CORRECT. The text states that European powers \"justified their colonization efforts by claiming to bring civilization, Christianity, and economic development to the continent.\" \\n\\nClaim 2: European powers justified their colonization efforts by bringing Islam to the continent. \\n\\nAccuracy: INCORRECT. The text states that European powers \"justified their colonization efforts by claiming to bring civilization, Christianity, and economic development to the continent.\" Islam is not mentioned as one of the claims made by European powers.\n",
        "      Final Answer: INCORRECT\n",
        "\n",
        "      <CONTEXT>\n",
        "      {context}\n",
        "      </CONTEXT>\n",
        "      <RESPONSE>\n",
        "      {sentence}\n",
        "      </RESPONSE>\n",
        "      Evaluation:\n",
        "      \"\"\"\n",
        "      evaluation = self.try_n_times(correctness, lambda x: x.split(\"\\n\")[-1].split(\" \")[-1].strip() == \"CORRECT\", 3)\n",
        "      if not evaluation:\n",
        "        incorrect_sentences.append(sentence)\n",
        "      correct = correct and evaluation\n",
        "    prompt = f\"\"\"Does the response answer the question? Remember that a response with incorrect information can still answer the question if the misinformation does not overly impact the main points of the response. Return one word YES or NO:\n",
        "    Example1:\n",
        "    <QUESTION>\n",
        "    </QUESTION>\n",
        "    <RESPONSE>\n",
        "    </RESPONSE>\n",
        "    Answer: YES\n",
        "\n",
        "    <QUESTION>\n",
        "    {q}\n",
        "    </QUESTION>\n",
        "    <RESPONSE>\n",
        "    {student_response}\n",
        "    </RESPONSE>\n",
        "    \"\"\"\n",
        "    yesno = self.try_n_times(prompt, lambda x: x.split(\"\\n\")[-1].split(\" \")[-1].strip() == \"YES\", 2)\n",
        "    return (correct, incorrect_sentences, yesno)\n",
        "\n",
        "  def evaluate_core_standard(self, context, student_response):\n",
        "    prompt = f\"\"\"\n",
        "    {self.rubric}\n",
        "    Score the following response from 1-3. Remember the student can only\n",
        "    use information provided in the context and scale the score accordingly. Explain why step by step.\n",
        "    Then, unless the score is 3, give suggestions on how to improve the score.\n",
        "    <CONTEXT>\n",
        "    {context}\n",
        "    </CONTEXT>\n",
        "    <RESPONSE>\n",
        "    {student_response}\n",
        "    </RESPONSE>\n",
        "    \"\"\"\n",
        "    feedback = self.t_chain.predict(task=prompt)\n",
        "    return feedback\n",
        "\n",
        "  def evaluate_response(self, context, question, student_response, max_edits=2):\n",
        "    correct, incorrect_sentences, answered_question = self.evaluate_correctness(context, question, student_response)\n",
        "    standard_feedback = self.evaluate_core_standard(context, student_response)\n",
        "    if correct and answered_question:\n",
        "      feedback = \"Good Job! You answered the question correctly.\\n\"\n",
        "    elif correct and not answered_question:\n",
        "      feedback = \"Your response was accurate, but did not adequately answer the question.\\n\"\n",
        "      prompt = f\"Explain step by step why the response does not adequately answer the question: {question}\"\n",
        "      feedback += self.t_chain.predict(task=prompt) + \"\\n\"\n",
        "    elif not correct and answered_question:\n",
        "      feedback = \"Your response contained incorrect information but overall still answered the question.\\n\"\n",
        "      for sentence in incorrect_sentences:\n",
        "        prompt = f\"Explain step by step why the sentence '{sentence}' is incorrect given the provided context\"\n",
        "        feedback += self.t_chain.predict(task=prompt) + \"\\n\"\n",
        "    else:\n",
        "      feedback = \"Your response contained incorrect information and did not adequately answer the question.\\n\"\n",
        "      for sentence in incorrect_sentences:\n",
        "        prompt = f\"Explain step by step why the sentence '{sentence}' is incorrect given the provided context\"\n",
        "        feedback += self.t_chain.predict(task=prompt) + \"\\n\"\n",
        "      prompt = f\"Explain step by step why the response does not adequately answer the question: {question}\"\n",
        "      feedback += self.t_chain.predict(task=prompt) + \"\\n\"\n",
        "    feedback = feedback + \"\\n\\n\" + standard_feedback\n",
        "\n",
        "    passed_qa = False\n",
        "    num_edits = 0\n",
        "    while not passed_qa and num_edits <= max_edits and self.enabled_qa:\n",
        "      passed_qa, failed_check = self.check_feedback_quality(feedback)\n",
        "      if not passed_qa:\n",
        "        prompt = f\"\"\"Return an edited version of this feedback so that it does not {failed_check}.\n",
        "        <FEEDBACK>\n",
        "        {feedback}\n",
        "        </FEEDBACK>\n",
        "        \"\"\"\n",
        "        print(f\"Improving feedback so that it does not {failed_check}\")\n",
        "        num_edits += 1\n",
        "        feedback = self.quality_chain.predict(task=prompt)\n",
        "    return feedback\n",
        "\n",
        "TA = TeachingStaff(OPENAI_API_KEY, standard=\"Draw evidence from literary or informational texts to support analysis, reflection, and research.\", enable_qa=True)\n",
        "TA.set_topic(\"Dinosaur extinction event\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJtCjeXElzBT"
      },
      "outputs": [],
      "source": [
        "\n",
        "OPENAI_API_KEY=\"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\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pokxzi_lvv_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a336f29e-bb2d-42c6-efb0-cb8e4cea79c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dinosaur extinction event, often referred to as the Cretaceous-Paleogene (K-Pg) extinction event, occurred approximately 66 million years ago. This catastrophic event, widely believed to have been caused by a large asteroid or comet impact, led to the extinction of approximately 75% of all plant and animal species on Earth, including the dinosaurs.\n",
            "The dinosaur extinction event, or the Cretaceous-Paleogene (K-Pg) extinction event, is a defining moment in Earth's history. This event, which occurred approximately 66 million years ago, marks the end of the Mesozoic Era, also known as the Age of Reptiles. The event was so devastating that it led to the extinction of approximately 75% of all plant and animal species on Earth, including the dominant terrestrial vertebrates at the time, the dinosaurs.\n",
            "\n",
            "The primary cause of this event is widely believed to be the impact of a large asteroid or comet, estimated to be 6 to 9 miles (10 to 15 kilometers) in diameter. The impact site, known as the Chicxulub crater, has been located near the Yucatán Peninsula in Mexico. The energy released by the impact was staggering, equivalent to over a billion Hiroshima atomic bombs, leading to catastrophic consequences including massive tsunamis, wildfires, and a \"nuclear winter\" effect caused by dust and debris blocking sunlight.\n",
            "\n",
            "The evidence for this event is found in a distinct layer of clay enriched with the rare element iridium, which is found in higher concentrations in asteroids and comets than on Earth. This iridium layer is found in geological deposits all around the globe and dates back precisely to the time of the extinction event. Despite the devastating effects of this event, not all life on Earth was wiped out. Some groups of animals, including mammals and birds, managed to survive, leading to their eventual dominance in the following era.\n",
            "\n",
            "What evidence is there in geological deposits that supports the widely accepted theory of the cause of the Cretaceous-Paleogene extinction event and how does this evidence connect to the survival of certain animal groups?\n"
          ]
        }
      ],
      "source": [
        "frq, question = TA.generate_FRQ()\n",
        "print(frq + \"\\n\\n\" + question)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "student_response = TA.generate_response(frq + \"\\n\\n\" + question)\n",
        "print(student_response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhRu9jZPI2LC",
        "outputId": "f65f7578-d724-4db7-bed1-4b457cb3873d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The evidence found in geological deposits that supports the widely accepted theory of the cause of the Cretaceous-Paleogene extinction event is a distinct layer of clay enriched with the rare element iridium. This element is usually found in higher concentrations in asteroids and comets than on Earth. This iridium layer is found in many different geological deposits all over the world and is dated right back to the time of the extinction event, which makes it a strong evidence for the asteroid or comet impact theory. \n",
            "\n",
            "This evidence connects to the survival of certain animal groups in a very interesting way. While the fallout from the asteroid or comet impact was devastating, it didn't wipe out all the life on Earth. Some animals, like mammals and birds, were somehow able to survive this catastrophic event. The theory is that these animals were able to adapt to the harsh conditions following the impact, like the \"nuclear winter\" effect caused by dust and debris blocking out sunlight. This would have resulted in a global drop in temperatures and a decrease in photosynthesis, making survival very difficult for many species. However, the mammals and birds that managed to survive this tough time were able to thrive and evolve in the new environment, eventually becoming the dominant species on Earth.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "student_response = \"\"\"The evidence found in geological deposits that supports the widely accepted theory of the cause of the Cretaceous-Paleogene extinction event is a distinct layer of clay enriched with the rare element iridium. This element is usually found in higher concentrations in asteroids and comets than on Earth. This iridium layer is found in many different geological deposits all over the world and is dated right back to the time of the extinction event, which makes it a strong evidence for the asteroid or comet impact theory. This evidence connects to the survival of certain animal groups in a very interesting way. While the fallout from the asteroid or comet impact was devastating, it didn't wipe out all the life on Earth. The theory is that these animals were able to adapt to the harsh conditions following the impact, like the \"nuclear winter\" effect caused by dust and debris blocking out sunlight. The T-Rex also survived. The mammals and birds that managed to survive this tough time were able to thrive and evolve in the new environment, eventually becoming the dominant species on Earth.\"\"\"\n",
        "feedback = TA.evaluate_response(frq, question, student_response)\n",
        "print(feedback)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-zL1kT-I3Xi",
        "outputId": "44e5ade0-1440-41e6-9d09-e8fddad2f0e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your response contained incorrect information but overall still answered the question.\n",
            "Step 1: The context provided details the Cretaceous-Paleogene (K-Pg) extinction event, which is known to have resulted in the extinction of approximately 75% of all plant and animal species on Earth, including dinosaurs.\n",
            "\n",
            "Step 2: The Tyrannosaurus Rex, commonly known as the T-Rex, is a species of dinosaur. As such, it falls under the category of organisms that were annihilated in the extinction event.\n",
            "\n",
            "Step 3: The context explicitly mentions that the event led to the extinction of the dinosaurs. Therefore, the statement \"The T-Rex also survived\" contradicts the information provided in the context.\n",
            "\n",
            "Step 4: The context also talks about the survival of certain animal groups, specifically mammals and birds. However, the T-Rex, being a dinosaur and not a mammal or bird, would not have been among the survivors.\n",
            "\n",
            "Step 5: Therefore, given the information in the context about the widespread extinction of dinosaurs during the K-Pg event, the sentence 'The T-Rex also survived.' is incorrect.\n",
            "\n",
            "\n",
            "Score: 2.5\n",
            "\n",
            "Explanation: The response shows a clear understanding of the text and makes good use of evidence to support its points, such as the iridium layer found in geological deposits and its connection to the asteroid or comet impact theory. The student also correctly identifies the survival of certain animal groups, including mammals and birds, as a result of the event. \n",
            "\n",
            "However, there are areas where the response could be improved. The student mentions that \"The T-Rex also survived,\" which is false information as the Tyrannosaurus Rex, like other dinosaurs, went extinct after the event. This mistake suggests a lack of thorough understanding or a misinterpretation of the context provided.\n",
            "\n",
            "Improvement suggestions:\n",
            "1. Make sure to verify all information before including it in the response. In this case, ensure to understand that the event led to the extinction of all dinosaurs, including the T-Rex.\n",
            "2. Provide a deeper analysis of the evidence. The student could discuss why the iridium layer is significant evidence of an asteroid or comet impact, and how the ability to adapt to harsh conditions allowed certain animal groups to survive and thrive.\n",
            "3. Reflect more on the implications of this event. For instance, how did the survival and subsequent dominance of mammals and birds shape the evolution of life on Earth?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deploy Web Demo"
      ],
      "metadata": {
        "id": "_jFsWp5eLzO7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QaSv8wwmn2d",
        "outputId": "84a46515-b6dc-4a25-e8e0-96d16fcd01a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        " %%writefile app.py\n",
        "import langchain\n",
        "import os, streamlit as st\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.cache import InMemoryCache\n",
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    PromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "\n",
        "frq_quality_rubric = [\"Can the question be answered using only evidence from the context above?\",\n",
        "                      \"Does the question test understanding of the information presented in the context?\"]\n",
        "feedback_quality_rubric = [\"contradict itself\",\n",
        "                           \"repeat the same points\"]\n",
        "\n",
        "class TeachingStaff:\n",
        "\n",
        "  def __init__(self, api_key, standard, topic=None, rubric=None, enable_qa=False):\n",
        "    chatllm = ChatOpenAI(openai_api_key=api_key, model_name='gpt-4') #model_name='gpt-4'\n",
        "    self.llm = OpenAI(openai_api_key=api_key, model_name='gpt-4') #model_name='gpt-4'\n",
        "\n",
        "    if topic != None:\n",
        "      self.set_topic(topic)\n",
        "    self.standard = standard\n",
        "\n",
        "    self.knowledge_gen_chain, self.question_asking_chain, \\\n",
        "    self.t_chain, self.s_chain, self.quality_chain = self.initialize_agents(chatllm)\n",
        "\n",
        "    if rubric != None:\n",
        "      self.rubric = rubric\n",
        "    else:\n",
        "      self.rubric = self.generate_rubric()\n",
        "\n",
        "    self.enabled_qa = enable_qa\n",
        "\n",
        "  def set_topic(self, topic):\n",
        "    self.topic = topic\n",
        "\n",
        "  def get_rubric(self):\n",
        "    return self.rubric\n",
        "\n",
        "  def initialize_agents(self, llm):\n",
        "    # knowledge generation chain\n",
        "    kg_template = \"\"\"Mary is an super-intelligent, advanced AI task executor that posseses accurate knowledge on every topic.\n",
        "     It does not mention itself or admit its nature as an AI.\n",
        "     It uses the voice of a primary or secondary source.\n",
        "     It does not use first or second person except when quoting a source.\n",
        "     It fulfills requests exactly and concisely\n",
        "     For the following requests, you will respond and do tasks as Mary.\n",
        "\n",
        "     {chat_history}\n",
        "     Human: {task}\n",
        "     Mary:\"\"\"\n",
        "    kg_prompt = PromptTemplate(input_variables=[\"chat_history\", \"task\"], template=kg_template)\n",
        "    kg_memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "    kg_chain = LLMChain(llm=llm, prompt=kg_prompt, verbose=False, memory=kg_memory)\n",
        "\n",
        "    # question asking chain\n",
        "    qa_template = f\"\"\"John is an super-intelligent question asking AI with critical reading and thinking skills.\n",
        "     It does not mention itself or admit its nature as an AI.\n",
        "     It does not use first or second person except when quoting a source.\n",
        "     It specializes in generating insightful questions that test understanding of a text passage.\n",
        "     Its questions expect the response to exhibit the standard: {self.standard}.\n",
        "     For the following requests, you will respond and do tasks as John.\\n\\n\"\"\" + \\\n",
        "     \"\"\"\n",
        "     {chat_history}\n",
        "     Human: {task}\n",
        "     John:\"\"\"\n",
        "    qa_prompt = PromptTemplate(input_variables=[\"chat_history\", \"task\"], template=qa_template)\n",
        "    qa_memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "    qa_chain = LLMChain(llm=llm, prompt=qa_prompt, verbose=False, memory=qa_memory)\n",
        "\n",
        "    # teaching chain\n",
        "    t_template = \"\"\"Susan is an super-intelligent, advanced educational AI that is an expert at teaching students.\n",
        "     It is knowledgable in all common student mistakes.\n",
        "     It does not mention itself or admit its nature as an AI.\n",
        "     It does not use first or second person except when quoting a source.\n",
        "     It fulfills requests exactly and concisely\n",
        "     It is extremely competent in evaluating free response questions, identifying false information,\n",
        "     and providing the best advice for students to improve their writing with respect to a rubric.\n",
        "     For the following requests, you will respond and do tasks as Susan.\n",
        "\n",
        "     {chat_history}\n",
        "     Human: {task}\n",
        "     Susan:\"\"\"\n",
        "    t_prompt = PromptTemplate(input_variables=[\"chat_history\", \"task\"], template=t_template)\n",
        "    t_memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "    t_chain = LLMChain(llm=llm, prompt=t_prompt, verbose=False, memory=t_memory)\n",
        "\n",
        "    # student chain (simulate student for testing)\n",
        "    s_template = \"\"\"Zach is a human 4th grade student that is doing a writing assignment.\n",
        "    For the following requests, you will respond and write as Zach.\n",
        "\n",
        "    {chat_history}\n",
        "    {task}\n",
        "    Zach:\"\"\"\n",
        "    s_prompt = PromptTemplate(input_variables=[\"chat_history\", \"task\"], template=s_template)\n",
        "    s_memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "    s_chain = LLMChain(llm=llm, prompt=s_prompt, verbose=False, memory=s_memory)\n",
        "\n",
        "    # quality assurance agent\n",
        "    quality_template = \"\"\"Jan is an advanced teaching quality assurance AI that is an expert at\n",
        "    editing educational text to promote concise, helpful, and easy to understand material for students.\n",
        "    It does not mention itself or admit its nature as an AI.\n",
        "    It fulfills requests exactly and concisely without repeating the request.\n",
        "    It does not use first or second person except when quoting a source.\n",
        "    For the folling requests, you will respond as Jan.\n",
        "\n",
        "    {chat_history}\n",
        "    {task}\n",
        "    Jan:\"\"\"\n",
        "    quality_prompt = PromptTemplate(input_variables=[\"chat_history\", \"task\"], template=quality_template)\n",
        "    quality_memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "    quality_chain = LLMChain(llm=llm, prompt=quality_prompt, verbose=False, memory=quality_memory)\n",
        "\n",
        "    return kg_chain, qa_chain, t_chain, s_chain, quality_chain\n",
        "\n",
        "  def generate_FRQ(self, max_retry=2):\n",
        "    intro, context = self.generate_intro_context()\n",
        "    question = self.generate_question(context)\n",
        "    passed_qa = False\n",
        "    tries = 0\n",
        "    while(not passed_qa and self.enabled_qa and tries <= max_retry):\n",
        "      passed_qa, failed_check = self.check_FRQ_quality(intro + \"\\n\" + context, question)\n",
        "      if not passed_qa:\n",
        "        print(\"discarded question: \" + question + \"\\n due to failing to pass: \" + failed_check + \"\\n\")\n",
        "        question = self.generate_question(context)\n",
        "        tries = tries + 1\n",
        "\n",
        "    return intro + \"\\n\" + context, question\n",
        "\n",
        "  def check_FRQ_quality(self, context, question):\n",
        "    prompt = f\"\"\"Refer to the following context and question for the following requests:\n",
        "    <CONTEXT>\n",
        "    {context}\n",
        "    </CONTEXT>\n",
        "    <QUESTION>\n",
        "    {question}\n",
        "    </QUESTION>\n",
        "    \"\"\"\n",
        "    self.quality_chain.predict(task=prompt)\n",
        "\n",
        "    for check in frq_quality_rubric:\n",
        "      if (\"YES\" not in self.quality_chain.predict(task=check + \" Answer with one word YES or NO.\")):\n",
        "        return False, check\n",
        "    return True, \"\"\n",
        "\n",
        "  def check_feedback_quality(self, feedback):\n",
        "    prompt = f\"\"\"Refer to the following feedback for the following requests:\n",
        "    <FEEDBACK>\n",
        "    {feedback}\n",
        "    </FEEDBACK>\n",
        "    \"\"\"\n",
        "    self.quality_chain.predict(task=prompt)\n",
        "\n",
        "    for check in feedback_quality_rubric:\n",
        "      if (\"YES\" in self.quality_chain.predict(task=\"Does the feedback \" + check + \"? Answer with one word YES or NO.\")):\n",
        "        return False, check\n",
        "    return True, \"\"\n",
        "\n",
        "  def generate_intro_context(self):\n",
        "    intro_prompt = f\"Write a short 2 sentence introduction for the topic '{self.topic}'\"\n",
        "    intro = self.knowledge_gen_chain.predict(task=intro_prompt)\n",
        "    knowledge_prompt = f\"generate 8 facts related to the topic '{self.topic}' which could follow your introduction\"\n",
        "    knowledge = self.knowledge_gen_chain.predict(task=knowledge_prompt)\n",
        "    context_prompt = f\"generate 3 paragraphs about the topic naturally utilizing the above facts incorporating quotes if necessary\"\n",
        "    context = self.knowledge_gen_chain.predict(task=context_prompt)\n",
        "    return intro, context\n",
        "\n",
        "  def generate_question(self, context):\n",
        "    prompt = f\"Generate an open-ended question which can be answered solely by drawing evidence from the context:\\n'{context}'\"\n",
        "    question = self.question_asking_chain.predict(task=prompt)\n",
        "    return question\n",
        "\n",
        "  def generate_rubric(self):\n",
        "    prompt = \\\n",
        "    f\"\"\"Concisely generate a rubric for grading an essay answer based on how well it demonstrates the core standard '{self.standard}'.\n",
        "    It should assign a score from 1-3 and give criteria for meeting each score cutoff. \"\"\"\n",
        "    return self.t_chain.predict(task=prompt)\n",
        "\n",
        "  def generate_response(self, frq):\n",
        "    prompt = f\"Answer the following question in paragraph form:\\n {frq}\"\n",
        "    return self.s_chain.predict(task=prompt)\n",
        "\n",
        "  def try_n_times(self, prompt, condition, n):\n",
        "    for i in range(0, n):\n",
        "      if condition(self.llm(prompt)):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "  def evaluate_correctness(self, context, q, student_response):\n",
        "    sentences = sent_tokenize(student_response.strip())\n",
        "    correct = True\n",
        "    incorrect_sentences = []\n",
        "    for sentence in sentences:\n",
        "      correctness = f\"\"\"Identify claims from the response and and evaluate accuracy of each using evidence from the context step by step. Evidence from the context is not needed if the claim is common sense. Return a final answer of CORRECT if all claims are accurate, and INCORRECT otherwise..\n",
        "      Example1:\n",
        "      <CONTEXT>\n",
        "      </CONTEXT>\n",
        "      <RESPONSE>\n",
        "      During the Scramble for Africa, European powers justified their colonization efforts by claiming to bring civilization, Islam, and economic development to the continent.\n",
        "      </RESPONSE>\n",
        "      Evaluation:\n",
        "      Claim 1: European powers justified their colonization efforts by bringing civilization to the continent. \\n\\nAccuracy: CORRECT. The text states that European powers \"justified their colonization efforts by claiming to bring civilization, Christianity, and economic development to the continent.\" \\n\\nClaim 2: European powers justified their colonization efforts by bringing Islam to the continent. \\n\\nAccuracy: INCORRECT. The text states that European powers \"justified their colonization efforts by claiming to bring civilization, Christianity, and economic development to the continent.\" Islam is not mentioned as one of the claims made by European powers.\n",
        "      Final Answer: INCORRECT\n",
        "\n",
        "      <CONTEXT>\n",
        "      {context}\n",
        "      </CONTEXT>\n",
        "      <RESPONSE>\n",
        "      {sentence}\n",
        "      </RESPONSE>\n",
        "      Evaluation:\n",
        "      \"\"\"\n",
        "      evaluation = self.try_n_times(correctness, lambda x: x.split(\"\\n\")[-1].split(\" \")[-1].strip() == \"CORRECT\", 3)\n",
        "      if not evaluation:\n",
        "        incorrect_sentences.append(sentence)\n",
        "      correct = correct and evaluation\n",
        "    prompt = f\"\"\"Does the response answer the question? Remember that a response with incorrect information can still answer the question if the misinformation does not overly impact the main points of the response. Return one word YES or NO:\n",
        "    Example1:\n",
        "    <QUESTION>\n",
        "    </QUESTION>\n",
        "    <RESPONSE>\n",
        "    </RESPONSE>\n",
        "    Answer: YES\n",
        "\n",
        "    <QUESTION>\n",
        "    {q}\n",
        "    </QUESTION>\n",
        "    <RESPONSE>\n",
        "    {student_response}\n",
        "    </RESPONSE>\n",
        "    \"\"\"\n",
        "    yesno = self.try_n_times(prompt, lambda x: x.split(\"\\n\")[-1].split(\" \")[-1].strip() == \"YES\", 2)\n",
        "    return (correct, incorrect_sentences, yesno)\n",
        "\n",
        "  def evaluate_core_standard(self, context, student_response):\n",
        "    prompt = f\"\"\"\n",
        "    {self.rubric}\n",
        "    Score the following response from 1-3. Remember the student can only\n",
        "    use information provided in the context and scale the score accordingly. Explain why step by step.\n",
        "    Then, unless the score is 3, give suggestions on how to improve the score.\n",
        "    <CONTEXT>\n",
        "    {context}\n",
        "    </CONTEXT>\n",
        "    <RESPONSE>\n",
        "    {student_response}\n",
        "    </RESPONSE>\n",
        "    \"\"\"\n",
        "    feedback = self.t_chain.predict(task=prompt)\n",
        "    return feedback\n",
        "\n",
        "  def evaluate_response(self, context, question, student_response, max_edits=2):\n",
        "    correct, incorrect_sentences, answered_question = self.evaluate_correctness(context, question, student_response)\n",
        "    standard_feedback = self.evaluate_core_standard(context, student_response)\n",
        "    if correct and answered_question:\n",
        "      feedback = \"Good Job! You answered the question correctly.\\n\"\n",
        "    elif correct and not answered_question:\n",
        "      feedback = \"Your response was accurate, but did not adequately answer the question.\\n\"\n",
        "      prompt = f\"Explain step by step why the response does not adequately answer the question: {question}\"\n",
        "      feedback += self.t_chain.predict(task=prompt) + \"\\n\"\n",
        "    elif not correct and answered_question:\n",
        "      feedback = \"Your response contained incorrect information but overall still answered the question.\\n\"\n",
        "      for sentence in incorrect_sentences:\n",
        "        prompt = f\"Explain step by step why the sentence '{sentence}' is incorrect given the provided context\"\n",
        "        feedback += self.t_chain.predict(task=prompt) + \"\\n\"\n",
        "    else:\n",
        "      feedback = \"Your response contained incorrect information and did not adequately answer the question.\\n\"\n",
        "      for sentence in incorrect_sentences:\n",
        "        prompt = f\"Explain step by step why the sentence '{sentence}' is incorrect given the provided context\"\n",
        "        feedback += self.t_chain.predict(task=prompt) + \"\\n\"\n",
        "      prompt = f\"Explain step by step why the response does not adequately answer the question: {question}\"\n",
        "      feedback += self.t_chain.predict(task=prompt) + \"\\n\"\n",
        "    feedback = feedback + \"\\n\\n\" + standard_feedback\n",
        "\n",
        "    passed_qa = False\n",
        "    num_edits = 0\n",
        "    while not passed_qa and num_edits <= max_edits and self.enabled_qa:\n",
        "      passed_qa, failed_check = self.check_feedback_quality(feedback)\n",
        "      if not passed_qa:\n",
        "        prompt = f\"\"\"Return an edited version of this feedback so that it does not {failed_check}.\n",
        "        <FEEDBACK>\n",
        "        {feedback}\n",
        "        </FEEDBACK>\n",
        "        \"\"\"\n",
        "        print(f\"Improving feedback so that it does not {failed_check}\")\n",
        "        num_edits += 1\n",
        "        feedback = self.quality_chain.predict(task=prompt)\n",
        "    return feedback\n",
        "\n",
        "st.title(\"Automated Teaching Assistant\")\n",
        "\n",
        "OPENAI_API_KEY = st.text_input(\"OpenAI API Key\", type=\"password\")\n",
        "topic = st.text_input(\"Choose a learning topic\")\n",
        "core_standard = st.text_input(\"Provide a core learning standard\")\n",
        "\n",
        "if st.button(\"Generate Free Response Question About Topic\"):\n",
        "  if not OPENAI_API_KEY.strip() or not topic.strip() or not core_standard.strip():\n",
        "        st.write(f\"Please complete the missing fields.\")\n",
        "  else:\n",
        "      try:\n",
        "        TS = TeachingStaff(OPENAI_API_KEY, standard=core_standard, enable_qa=True)\n",
        "        TS.set_topic(topic)\n",
        "        context, question = TS.generate_FRQ()\n",
        "\n",
        "        st.session_state['TS'] = TS\n",
        "        st.session_state['context'] = context\n",
        "        st.session_state['question'] = question\n",
        "        st.session_state['rubric'] = TS.get_rubric()\n",
        "      except Exception as e:\n",
        "          st.write(f\"An error occurred: {e}\")\n",
        "try:\n",
        "  st.write(st.session_state['context'])\n",
        "  st.write(st.session_state['question'])\n",
        "  st.write(\"The following rubric will be used to evaluate your answer:\")\n",
        "  st.write(st.session_state['rubric'])\n",
        "except:\n",
        "  pass\n",
        "\n",
        "if st.button(\"Use ChatGPT to generate a model answer!\"):\n",
        "  try:\n",
        "    AI_response = st.session_state['TS'].generate_response(st.session_state['context'] + \"\\n\\n\" + st.session_state['question'])\n",
        "    st.session_state['AI_response'] = AI_response\n",
        "  except Exception as e:\n",
        "    st.write(f\"An error occurred: {e}\")\n",
        "\n",
        "try:\n",
        "  st.write(st.session_state['AI_response'])\n",
        "except:\n",
        "  pass\n",
        "\n",
        "student_response = st.text_area(\"Answer here\", height=200)\n",
        "if st.button(\"Evaluate my answer!\"):\n",
        "  if not student_response.strip():\n",
        "    st.write(f\"Please complete the missing fields.\")\n",
        "  else:\n",
        "    try:\n",
        "      feedback = st.session_state['TS'].evaluate_response(st.session_state['context'], st.session_state['question'], student_response)\n",
        "      st.session_state['feedback'] = feedback\n",
        "    except Exception as e:\n",
        "      st.write(f\"An error occurred: {e}\")\n",
        "try:\n",
        "  st.write(st.session_state['feedback'])\n",
        "except:\n",
        "  pass\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"import urllib\n",
        "# Paste returned IP address into the field on the url generated by local tunnel to access the web demo\n",
        "print(\"Password/Enpoint IP for localtunnel is:\",urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))"
      ],
      "metadata": {
        "id": "2f1uxoMIZcXJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9912354-337b-438f-a14f-ab7025ac09ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Password/Enpoint IP for localtunnel is: 35.231.155.142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/content/logs.txt &"
      ],
      "metadata": {
        "id": "qaQM41rwMJw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHXbuKhmMdhD",
        "outputId": "e3f88291-69d0-4659-c537-b9405c9d07e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 4.378s\n",
            "your url is: https://beige-heads-drum.loca.lt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gUS02Sa0VSnL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMiT1+aDfoliUmwlu+GdIpB",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}